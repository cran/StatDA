\documentclass{report}
%\VignetteIndexEntry{StatDA}
\newcommand{\StatDA}{{\tt StatDA }}
\pagestyle{headings}

\pagetotal=29.7cm \textwidth=15.0cm \textheight=22.1cm
\topmargin=-1.0cm \headheight=0.3cm \headsep=1.6cm
\oddsidemargin=0.40cm \evensidemargin=0.40cm \arraycolsep=2pt
\columnsep=0.60cm
\usepackage{rotating}
\title{Tutorial to the \\ package StatDA}
\author{Peter Filzmoser\\[2mm]
Institute of Statistics and Probability Theory\\
Vienna University of Technology, Austria}

\begin{document}



\maketitle
\tableofcontents
@
\chapter{Introduction}
This document intends to show the main features of the package \StatDA.
The reader should get an idea about the main methods that are implemented,
and about their use within R.

\StatDA uses mainly examples from environmental studies. In fact, here
mainly the Kola data set is used for demonstrating the methods. This data set resulted from a larger geochemical study carried out at the
peninsula Kola in Northern Europe. A detailed description of the data set
can be found in the book ``Statistical Data Analysis Explained. Applied Environmental Statistics with R.'' \footnote{C. Reimann, P. Filzmoser, R.G. Garrett, R. Dutter; Wiley, Chichester, 2008}. This book is also the basis
for the package \StatDA. It described all the methods in detail,
shows various applications of the methods, and explains the results.

Of course, \StatDA can also be used for other kind of data. The methods
are mostly standard methods used for analyzing uni- and multivariate data.
Many methods with focus on exploratory data analysis are included, because
getting an appropriate view on the data can be very valuable in deepening
the knowledge about the data structure.

The web page
\begin{quote}
http://www.statistik.tuwien.ac.at/StatDA/R-scripts
\end{quote}
can also be helpful--especially for the non-experienced R user--to make
first steps of analyzing data with R. There one can find all the figures
included in the previously mentioned book, together with all R-code for
generating the figures. With copy/paste of the code into an R-session
it will be possible to reproduce the plots exactly. Including your own
data instead of the data used in the book will allow for a perfect
approach in analyzing your data. 



\chapter{Graphics to Display the Data Distribution}
The Kola project includes the chemical analysis for ca. 50 chemical elements at about 600 sample sites. This large amount of information needs to be summarised. First of all it is important to know which distributions the data follow. Therefore some basic statistical plots can be made. The easiest plots are the histogram, density plot, scatterplot and a boxplot. The graphics can look very different for the different samples. For example, the distribution can be symmetrical or asymmetrical. The data can fall into two groups or single extreme outliers can appear.

It is not necessary to explain those standard tools of statistical analysis. It is more important to mention other graphics which compare the empirical distribution with an underlying distribution. The well known plots for such graphics are the QQ-plot, QP-plot and the PP-plot. In this chapter the main focus will be on the boxplot and its problems with outlier detection in the case of non normally distributed data sets. The displayed graphic includes the histogram, density trace, scatterplot, boxplot and the Empirical Cumulative Distribution Function (ECDF) of the original data (upper graphics) and the log-transformed data (lower graphics).
The curve of the ECDF plot 
jumps witch each data point by $1/n$, where $n$ is the number of data points. The x-axis shows the variable values and the y-axis the corresponding probabilities of the empirical cumulative distribution function (values between 0 and 1).

The package \StatDA loads several other packages which are automatically installed
when installing \StatDA. Loading \StatDA in an R session with
\verb|library(StatDA)| will thus give the message for some of the additional
packages, that they are also loaded. The commands for generating some plots
for graphically inspecting the data are given below.

<<label=fig1plot,include=FALSE,echo=FALSE>>=
library(StatDA)
data(chorizon)
Ba=chorizon[,"Ba"]
n=length(Ba)
par(mfcol=c(2,2),mar=c(4,4,2,2))
edaplotlog(Ba,H.freq=F,box=T,H.breaks=30,S.pch=3,S.cex=0.5,D.lwd=1.5,P.log=F,
  P.main="",P.xlab="Ba [mg/kg]",P.ylab="Density",B.pch=3,B.cex=0.5,B.log=TRUE)
edaplot(log10(Ba),H.freq=F,box=T,S.pch=3,S.cex=0.5,D.lwd=1.5,P.ylab="Density",
   P.log=T,P.logfine=c(5,10),P.main="",P.xlab="Ba [mg/kg]",B.pch=3,B.cex=0.5)
plot(sort(Ba),((1:n)-0.5)/n,pch=3,cex=0.8,
main="",xlab="Ba [mg/kg]",ylab="Probability",cex.lab=1,cex.lab=1.4)
abline(h=seq(0,1,by=0.1),lty=3,col=gray(0.5))
abline(v=seq(0,1400,by=200),lty=3,col=gray(0.5))
plot(sort(log10(Ba)),((1:n)-0.5)/n,pch=3,cex=0.8,
main="",xlab="Ba [mg/kg]",ylab="Probability",cex.lab=1,xaxt="n",cex.lab=1.4)
axis(1,at=log10(alog<-sort(c((10^(-50:50))%*%t(c(5,10))))),labels=alog)
abline(h=seq(0,1,by=0.1),lty=3,col=gray(0.5))
abline(v=log10(alog),lty=3,col=gray(0.5))
@

\begin{figure}
\begin{center}
<<label=fig1,fig=TRUE,echo=TRUE>>=
<<fig1plot>>
@
\end{center}
\caption{Histogram, density trace, scatterplot, and boxplot in one display combined with the ECDF-plot.Original data (upper diagrams) and log-transformed data (lower diagrams)}
\label{fig:one}
\end{figure}

An important plot is the log-boxplot. The boxplot is built around the median which divides any sorted data set into two equal halves. The length of the box is determined by the first (25\%) and third (75\%) quartile. The distance of these quartiles is the IQR (interquartile range), the from each side of the box 1.5 times IQR is considered. Values being further away are potential outliers. The so-called whiskers are drawn to the most extreme data points within this range (i.e. the largest non-outliers). The problem is that the calculation of the whiskers of the boxplot assumes data symmetry. Thus also the recognition of extreme values is based on symmetry at the central data. In geochemistry we are often faced with right-skewed data distributions and therefore the boxplot will underestimate the number of lower extreme values and overestimate the number of upper extreme values. A solution for this problem is that the data can be log-transformed and then the boxplot can be computed. The result are the values (median, quantiles, whiskers,...) for the log-transformed data. It is possible to back-transform these values. Median and quartiles would not change for original or log-transformed data, but the whiskers for identifying outliers will change. This version of the boxplot is called the log-boxplot.

\chapter{Statistical Distribution Measures}
The main point of view of this chapter are statistical measures of the distribution. The well known measures for the central value are the arithmetic mean, the geometric mean, the mode and the median. Robust measures like the median are useful for environmental data because the influence of outliers is reduced. Another possibility is to trim the most extreme data values. There are other methods for robust estimation such as a weighted estimator. If someone chooses this method, the data points far away from the centre of the distribution are down-weighted.

Other important values are the measures of spread. The well known values are the range, the Interquartile Range (IQR), the Standard Deviation (SD) and the Variance. One additional value is discussed in the book and this is the Median Absolute Deviation (MAD). The MAD is the robust equivalent to the SD. In contrast to the Standard Deviation the median is taken as the central value. Now the median of the absolute deviations around this median is computed. As it is easy to see, the MAD is robust against up to 50\% of extreme values.

Other measures of spread are based on the quantiles, quartiles and percentiles. There is also a measure which is independent from the magnitude of the data because it is usually expressed in percent. This percentage exists in a robust and non robust version. The non robust version is called the coefficient of variation (CV) and the other one is called the robust coefficient of variation (CVR). The CV is defined as SD divided by the arithmetic mean, and the CVR is defined as MAD divided by the median (expressed in \%).

In Table 3.1 the most important distribution measures are displayed for selected variables of the Kola Project moss data set.

<<out, echo=FALSE,results=tex>>=
"descriptive" <-
function (x)
{
    lab <- c("MIN", "Q1", "MEDIAN", "MEAN", "Q3", "MAX",
        "SD", "MAD", "CV %", "CVR %")
    #lab <- c("N", "Missings", "MIN", paste("Q_",quanlow <- c(0.02,0.05,0.1),sep=""),
    #    "Q1", "MEDIAN", "MEAN-log", "MEAN", "Q3", 
    #    paste("Q_",quanup <- c(0.9,0.95,0.98),sep=""), "MAX",
    #    "SD", "MAD", "IQR","CV", "CVR",
    #    "KS-norm", "SW-norm", "KS-lognorm", "SW-lognorm")
    if (missing(x)) {
        return(lab)
    }
    temp <- rep(0, length(lab))
    xt <- x[!is.na(x)]
    ix <- order(xt)
    n <- length(xt)
    if (!is.numeric(xt) || all(is.na(x))) {
        #return(c(n, rep(NA, length(lab) - 2), length(x) - length(xt)))
        return(c(rep(NA, length(lab) - 3), length(x) - length(xt)))
    }
    if (n == 1) {
        #return(c(n, xt[1], NA, rep(xt[1], 5), length(x) - length(xt)))
        return(c(NA, rep(xt[1], 5), length(x) - length(xt)))
    }
    else {
        #return(c(n, length(x) - length(xt), min(xt), quantile(xt,quanlow),
        return(c(min(xt), quantile(xt,0.25), median(xt), mean(xt), quantile(xt,0.75), max(xt),
            sd(xt), mad(xt), sd(xt)/mean(xt)*100, mad(xt)/median(xt)*100))
    }
}

"sumstats" <-
function (x, by)
{
    if (!missing(by)) {
        x <- cat.to.list(c(x), by)
    }
    if (!is.list(x) & !is.matrix(x))
        x <- matrix(x, ncol = 1)
    if (is.list(x)) {
        nrow <- length(x)
        out <- matrix(NA, nrow = nrow, ncol = length(descriptive()))
        dimnames(out) <- list(names(x), descriptive())
        for (j in (1:nrow)) {
            if (is.numeric(x[[j]])) {
                out[j,] <- descriptive(x[[j]])
            }
        }
        return(out)
    }
    if (is.matrix(x)) {
        nr <- ncol(x)
        out <- matrix(NA, nrow = nr, ncol = length(descriptive()))
        dimnames(out) <- list(dimnames(x)[[2]], descriptive())
        for (j in (1:nr)) {
            out[j, ] <- descriptive(x[, j])
        }
        return(out)
    }
}
data(moss)
out<-sumstats(moss[,20:58])

@

\begin{small}
<<label=tab1,echo=FALSE,results=tex,width=9,height=9>>=
library(xtable)
xtable(out,caption="Summary statistics for selected elements of the Kola moss data.", label="tab1")


@
\label{tab:one}
\end{small}

\chapter{Mapping Spatial Data}
Geochemical data, as well as many other data sets, are spatially dependent. This is a big challenge for analyzing the data, since the relation to the spatial coordinates can be taken into account. Thus we are not only interested in the statistical distribution of the data, but also in the geographical distribution, which can be represented in a map.
Many different methods exist to show this structure. Some of them look informative and others are featureless. It is surprising that scientists pay so little attention to the preparation of geochemical maps. There is no common effective method to display environmental data on a map. Our main focus in this chapter is to present some mapping methods.

\section{Mapping Geochemical Data with Proportional Dots}
This method is very easy to understand and results in a map which is easy to read. It is also one of the most popular ways for black and white maps. The main idea is that the absolute value of the sample is important. All data values will be represented by dots in the map. If one takes linearly growing dots there can be two problems. The first one is that too many dots are similar in size and it is difficult to distinguish different values. On the other hand it is possible that the maximum is very far away from the rest of the data and then one huge dot will appear while the remaining dots are very small.

An alternative is exponentially growing dots. It is true that the problem with one huge dot can also appear. Therefore one has to modify the method by using the same point size for, say, the smallest 10 percent and the largest 1 percent of the data. In between the dots grow exponentiallay with the result that the dot size slows down near the detection limit and becomes steeper towards the larger values. 

Figure~\ref{fig:two} shows the map of the Kola project area, and the information of As in C-horizon is shown with exponentially growing dots with limited lower and upper point size.

<<label=fig2plot,include=FALSE, echo=FALSE>>=
par(mfrow=c(1,1))
data(kola.background)
el=chorizon[,"As"]
X=chorizon[,"XCOO"]
Y=chorizon[,"YCOO"]

plot(X,Y,frame.plot=FALSE,xaxt="n",yaxt="n",xlab="",ylab="",type="n")
plotbg(map.col=c("gray","gray","gray","gray"),add.plot=T)

bubbleFIN(X,Y,el,S=9,s=2,plottitle="",legendtitle="As [mg/kg]", text.cex=0.60,
	legtitle.cex=0.70,ndigits=2)

text(min(X)+diff(range(X))*5/7,max(Y),"Exponentially",cex=0.70)
text(min(X)+diff(range(X))*5/7,max(Y)-diff(range(Y))/25,"growing dots",cex=0.70)

scalebar(761309,7373050,861309,7363050,shifttext=-0.5,shiftkm=4e4,sizetext=0.8)

Northarrow(362602,7818750,362602,7878750,362602,7838750,Alength=0.15,Aangle=15,Alwd=1.3,Tcex=1.6)
@

\begin{figure}
\begin{center}
<<label=fig2,fig=TRUE,echo=TRUE,width=7,height=7>>=
<<fig2plot>>
@
\end{center}
\caption{The distribution of As in C-horizon with exponentially growing dots.}
\label{fig:two}
\end{figure}

\section{Percentile Classes}
Another approach for plotting geochemical data are classes. In this case the data set will be grouped and the resulting classes will be plotted. To distiguish the different groups the user can select different coloured symbols, or black and white. One possibility is to use percentiles for grouping the values. This is an easy way because there is no assumption about the underlying data distribution. However, it is necessary to make a choice of the percentiles for grouping the data. When someone wants to spread the symbols across the range of values it is possible to use the $20^\textrm{th}, 40^\textrm{th}, 60^\textrm{th}, 80^\textrm{th}$ percentiles. Geochemists are often interested in the tails of the distribution and therefore they use the $5^\textrm{th}, 25^\textrm{th}, 75^\textrm{th}, 95^\textrm{th}$ percentiles. Further classes can also be added. Figure~\ref{fig:three} shows the map of the As in C-horizon for the above mentioned percentiles.

<<label=fig3plot,include=FALSE,echo=FALSE>>=
el=chorizon[,"As"]
X=chorizon[,"XCOO"]
Y=chorizon[,"YCOO"]
plot(X,Y,frame.plot=FALSE,xaxt="n",yaxt="n",xlab="",ylab="",type="n")
plotbg(map.col=c("gray","gray","gray","gray"),add.plot=T)

SymbLegend(X,Y,el,type="percentile",qutiles<-c(0,0.05,0.25,0.75,0.95,1),symbtype="EDA",symbmagn=0.8,
leg.position="topright",leg.title="As [mg/kg]",leg.title.cex=0.8,leg.round=2,leg.wid=4,leg.just="right")

text(min(X)+diff(range(X))*4/7,max(Y),paste(qutiles*100,collapse=","),cex=0.8)
text(min(X)+diff(range(X))*4/7,max(Y)-diff(range(Y))/25,"Percentiles",cex=0.8)

scalebar(761309,7373050,861309,7363050,shifttext=-0.5,shiftkm=37e3,sizetext=0.8)
Northarrow(362602,7818750,362602,7878750,362602,7838750,Alength=0.15,Aangle=15,Alwd=1.3,Tcex=1.6)
@

\begin{figure}
\begin{center}
<<label=fig3,fig=TRUE,echo=TRUE,width=7,height=7>>=
<<fig3plot>>
@
\end{center}
\caption{Geochemical map of the variable As}
\label{fig:three}
\end{figure}

Another possibility is to use the boxplot instead of percentiles for building the classes. The problem is the same as mentioned in the previous chapter. The boxplot identifies a reliable number of outliers if the data distribution is symmetric. Therefore it can be necessary to use another version of the boxplot such as the log-boxplot.

\section{Surface Maps Constructed with Smoothing Techniques}
Many users are interested in maps that look ``pleasing'', but for geochemist this representation has several disadvantages. Firstly, they loose information of the local variablility and secondly, the maps are dependend on the algorithm. It can also happen that the untrained user gets the impression of greater precision than the samples support. An easy way to produce surface maps is based on moving averages. To construct such a map the user has to define a window of fixed size. Now this window is moved across the map and all values inside the window will be represented by one central value (e.g. the median). With this method it is possible to estimate new values on the grid. This is a very simple method and there exist much more complex smoothing methods.

One possibility is to use a polynomial function for interpolation. This method results in a smoother display of the map but the number of classes is a critical step. This method has the same problems as a representation with proportional dots. This can be solved by using 100 percentile classes, each with its own grey value. If the user wants less classes he should choose the groups according to the data (e.g. $5^\textrm{th}, 25^\textrm{th}, 75^\textrm{th}, 95^\textrm{th}$ percentiles). Figure~\ref{fig:four} shows a smoothing with a polynomial function with 100 percentiles.

<<label=fig4plot, include=FALSE,echo=FALSE>>=
X=chorizon[,"XCOO"]
Y=chorizon[,"YCOO"]
el=log10(chorizon[,"As"])
data(kola.background)
data(bordersKola)

plot(X,Y,frame.plot=FALSE,xaxt="n",yaxt="n",xlab="",ylab="",type="n")

SmoothLegend(X,Y,el,resol=200,type="contin",whichcol="gray",
    qutiles=c(0,0.05,0.25,0.50,0.75,0.90,0.95,1),borders="bordersKola",
    leg.xpos.min=7.8e5,leg.xpos.max=8.0e5,leg.ypos.min=77.6e5,leg.ypos.max=78.7e5,
    leg.title="mg/kg", leg.title.cex=0.7, leg.numb.cex=0.7, leg.round=2,leg.wid=4,
    leg.numb.xshift=0.7e5,leg.perc.xshift=0.4e5,leg.perc.yshift=0.2e5,tit.xshift=0.35e5)

plotbg(map.col=c("gray","gray","gray","gray"),map.lwd=c(1,1,1,1),add.plot=T)

text(min(X)+diff(range(X))*4/7,max(Y),"As",cex=1)
text(min(X)+diff(range(X))*4/7,max(Y)-diff(range(Y))/28,"in C-horizon",cex=0.8)

scalebar(761309,7373050,861309,7363050,shifttext=-0.5,shiftkm=37e3,sizetext=0.8)
Northarrow(362602,7818750,362602,7878750,362602,7838750,Alength=0.15,Aangle=15,Alwd=1.3,Tcex=1.6)
@
\begin{figure}
\begin{center}
<<label=fig4, fig=TRUE,echo=TRUE,width=7,height=7>>=
<<fig4plot>>
@
\end{center}
\caption{Smoothed surface map of As in Kola C-horizon using 100 percentiles.}
\label{fig:four}
\end{figure}

\section{Surface Maps Constructed With Kriging}
Kriging is a statistically optimised estimator for interpolation. Many different methods exist for kriging, like block kriging (interpolation in a whole block) and point kriging (interpolation for any location in the space). The main idea is that points in the neighbourhood have more in common than points with a greater distance. The cruical point is to weight the different distances.

The basis for kriging is the (semi)variogram. This variogram displays the variance between data points at defined distances, and the distance is the Euclidean distance in the survey area. The maximum on the $x$-axis is 30 to 50 percent of the maximum distance in the survey area. In the case of the Kola project the maximum distance in the survey area is about 700 km. With the Euclidean distance any direction is possible but sometimes only one direction can be interesting. Therefore the distances can be calculated in only one direction, e.g. north-south or east-west. The omnidirectional variogram is a kind of average over all possible directions. In Figure~\ref{fig:five} (left) the variogram for different directions is displayed, and the right graphic shows the so called spherical variogram that can be used as a model for kriging.

\newpage
Only the important part of the code is shown below:
<<label=fig5imp,include=FALSE, echo=TRUE, results=hide>>=
X=chorizon[,"XCOO"]/1000
Y=chorizon[,"YCOO"]/1000
el=chorizon[,"As"]
vario.b <- variog(coords=cbind(X,Y), data=el, lambda=0, max.dist=300)
vario.c <- variog(coords=cbind(X,Y), data=el, lambda=0, max.dist=300, op="cloud")
vario.bc <- variog(coords=cbind(X,Y), data=el, lambda=0, max.dist=300, bin.cloud=TRUE)
vario.s <- variog(coords=cbind(X,Y), data=el, lambda=0, max.dist=300, op="sm", band=10)
vario.0 <- variog(coords=cbind(X,Y), data=el, lambda=0, max.dist=300, dir=0, tol=pi/8)
vario.90 <- variog(coords=cbind(X,Y), data=el, lambda=0, max.dist=300, dir=pi/4, tol=pi/8)
vario.45 <- variog(coords=cbind(X,Y), data=el, lambda=0, max.dist=300, dir=pi/8, tol=pi/8)
vario.120 <- variog(coords=cbind(X,Y), data=el, lambda=0, max.dist=300, dir=3*pi/8, tol=pi/8)
data(res.eyefit.As_C)
v5=variofit(vario.b,res.eyefit.As_C,cov.model="spherical",max.dist=300)
@

<<label=fig5plot,include=FALSE, echo=FALSE,results=hide>>=
par(mfrow=c(1,2),mar=c(4,4,2,2))
plot(0,0,xlab="Distance [km]",ylab="Semivariogram",xlim=c(0,300),ylim=c(0,1.6),
     cex.lab=1.2,type="n")
title("As in C-horizon",cex.main=1.2)
lines(vario.b,pch=1,type="p")
lines(vario.0,lty=1,pch=2)
lines(vario.90,lty=2,pch=3)
lines(vario.45,lty=3,pch=4)
lines(vario.120,lty=4,pch=5)
legend("bottomright", legend=c("omnidirectional","N-S","E-W","NW-SE","NE-SW"), pch=c(1,2,3,4,5))
data(res.eyefit.As_C)
plot(0,0,xlab="Distance [km]",ylab="Semivariogram",xlim=c(0,300),ylim=c(0,1.6),
     cex.lab=1.2,type="n")
title("As in C-horizon",cex.main=1.2)
lines(vario.b,pch=1,type="p")
lines(v5,col=1,lwd=2)
r=v5$cov.pars[2]
n=v5$nugget
s=v5$cov.pars[1]
arrows(0,0,0,n,length=0.08,code=3,col=gray(0.6))
text(2,n/2,paste("Nugget variance =",round(n,2)),cex=0.9,pos=4)
abline(h=n,col=gray(0.6),lty=2)
arrows(300,n,300,n+s,length=0.08,code=3,col=gray(0.6))
text(298,n+s/2,paste("Sill =",round(s,2)),cex=0.9,pos=2)
arrows(0,1.3,r,1.3,length=0.08,code=3,col=gray(0.6))
text(r/2,1.3,paste("Range =",round(r,0)),cex=0.9,pos=3)
@

\begin{figure}
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<label=fig5,fig=TRUE,echo=FALSE,width=9,height=4.5,results=hide>>=
<<fig5imp>>
<<fig5plot>>

@
\end{center}
\caption{Different directions (left) for the As in C-horizon samples and spherical semivariogram for the omnidirectional semivariogram (right).}
\label{fig:five}
\end{figure}

For further information we refer to the book. Now we have done all of the preparition to create a kriging plot of the map for the Kola project. As mentioned above it is possible to make kriging for a block or a single point. In our case we plot the interpolation of the map with block kriging. Whether the blocks are visible or not depends on the size of the block. For a better resolution smaller block sizes need to be used.

<<label=fig6plot,include=FALSE,echo=FALSE,results=hide>>==
par(mfrow=c(1,1))
X=chorizon[,"XCOO"]
Y=chorizon[,"YCOO"]
xwid=diff(range(X))/12e4
ywid=diff(range(Y))/12e4
el=chorizon[,"As"]
vario.b <- variog(coords=cbind(X,Y), data=el, lambda=0, max.dist=300000)
data(res.eyefit.As_C_m)
data(bordersKola)
v5=variofit(vario.b,res.eyefit.As_C_m,cov.model="spherical",max.dist=300000)
plot(X,Y,frame.plot=FALSE,xaxt="n",yaxt="n",xlab="",ylab="",type="n")
KrigeLegend(X,Y,el,resol=25,vario=v5,type="percentile",whichcol="gray",
    qutiles=c(0,0.05,0.25,0.50,0.75,0.90,0.95,1),borders="bordersKola",
  leg.xpos.min=7.8e5,leg.xpos.max=8.0e5,leg.ypos.min=77.6e5,leg.ypos.max=78.7e5,
    leg.title="mg/kg", leg.title.cex=0.7, leg.numb.cex=0.7, leg.round=2,
    leg.numb.xshift=0.7e5,leg.perc.xshift=0.4e5,leg.perc.yshift=0.2e5,tit.xshift=0.35e5)
plotbg(map.col=c("gray","gray","gray","gray"),map.lwd=c(1,1,1,1),add.plot=T)
text(min(X)+diff(range(X))*4/7,max(Y),"As",cex=1)
text(min(X)+diff(range(X))*4/7,max(Y)-diff(range(Y))/28,"in C-horizon",cex=0.8)
scalebar(761309,7373050,861309,7363050,shifttext=-0.5,shiftkm=37e3,sizetext=0.8)
Northarrow(362602,7818750,362602,7878750,362602,7838750,Alength=0.15,Aangle=15,Alwd=1.3,Tcex=1.6)
@

\begin{figure}
\begin{center}
<<label=fig6, fig=TRUE,echo=TRUE,width=7,height=7,results=hide>>=
<<fig6plot>>
@
\end{center}
\caption{Block kriging of As in C-horizon. The semivariogram in Figure~\ref{fig:five} (right) is used for kriging.}
\label{fig:six}
\end{figure}


\chapter{Further Graphics for Exploratory Data Analysis}
Another graphic to study the relationship between two different variables is the scatterplot. This plot is very easy to understand because the $x$-axis represents one variable and the $y$-axis represents the second variable. The result is a cloud corresponding to $n$ data points, and the shape of the cloud depends on the examined variables. After one has plotted the cloud it is possible to add a regression line to the graphic. The first step is to find a linear relation instead of an non-linear one. This is the situation were the theory of linear regression enters the field. To calculate a regression line one has two possibilities: a robust and a non-robust estimation. Basic information on the topic linear regression will be given later on.

The next step is to focus on a trend in different directions of the geochemical samples. Therefore we have to specify a special point and to determine the concentration in east-west or any other direction. The concentration can be plotted in a diagram where the $x$-axis represents the distance to the chosen point and the $y$-axis shows the concentration of the element. In a map one can plot the points which are studied. In Figure~\ref{fig:seven} (right) we can see that the Cu concentration decreases with the distance to Monchegorsk (the chosen point).

<<label=fig7plot,include=FALSE,echo=FALSE>>=
data(moss)
par(mfrow=c(1,2),mar=c(4,4,2,2))

plotbg(map.col=c("gray","gray","gray","gray"), xlab="UTM east [m]", ylab="UTM north [m]",
cex.lab=1.2)
X=moss[,"XCOO"]
Y=moss[,"YCOO"]
points(X[Y<7600000 & Y>7500000],Y[Y<7600000 & Y>7500000],pch=3,cex=0.7)
x=(X[Y<7600000 & Y>7500000]-753970)/1000
y=log10(moss[Y<7600000 & Y>7500000,"Cu"])
plot(x,y,xlab="Distance from Monchegorsk [km]",ylab="Cu in Moss [mg/kg]", yaxt="n",
     pch=3,cex=0.7, cex.lab=1.2)
axis(2,at=log10(a<-sort(c((10^(-50:50))%*%t(c(2,5,10))))),labels=a)
lines(smooth.spline(x,y),col=1,lwd=1.3)
@

\begin{figure}
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<label=fig7,fig=TRUE,echo=TRUE,width=9,height=4.5>>=
<<fig7plot>>
@
\end{center}
\caption{Points in the map used for the east-west transect (left). Plot which shows the decrease of the Cu concentration with larger distance to Monchegorsk.}
\label{fig:seven}
\end{figure}

Another possibility is to examine the concentration in one direction from the reference point. With this assumption it is possible to make an examination of the concentration in north-west to south-east direction. It is only necessary to use a subarea of the total data set.

In Figure~\ref{fig:eight} the reference point (Nikel/Zapojarnij) is in the north-west and the subarea goes to Monchegorsk in the south-east. In the plot on the right-hand side it is visible that near Nikel-Zapojarnij and Monchegorsk the concentration of Cu is higher than between the two places. The width of the peak says that the emissions from Monchegorsk have a larger spread.

<<label=fig8plot, include=FALSE, echo=FALSE>>=
library(sgeostat)
X=moss[,"XCOO"]
Y=moss[,"YCOO"]
Cu=moss[,"Cu"]
loc = list(x=c(590565.1,511872.8,779702.8,861156.3),
           y=c(7824652,7769344,7428054,7510341))
loc.in=in.polygon(X,Y,loc$x,loc$y) # observations in polygone
ref=c(542245.3,7803068) # reference point
par(mfrow=c(1,2),mar=c(4,4,2,2))
plotbg(map.col=c("gray","gray","gray","gray"), xlab="UTM east [m]", ylab="UTM north [m]",
cex.lab=1.2)
points(X[!loc.in],Y[!loc.in],pch=16,cex=0.3)
points(X[loc.in],Y[loc.in],pch=3,cex=0.7)
points(ref[1],ref[2],pch=16,cex=1)
polygon(loc$x,loc$y,border=1,lwd=1.3)
distanc=sqrt((X[loc.in]-ref[1])^2+(Y[loc.in]-ref[2])^2)
plot(distanc/1000,log10(Cu[loc.in]),xlab="Distance from reference point [km]",
       ylab="Cu in Moss [mg/kg]", yaxt="n", pch=3,cex=0.7, cex.lab=1.2)
axis(2,at=log10(a<-sort(c((10^(-50:50))%*%t(c(2,5,10))))),labels=a)
lines(smooth.spline(distanc/1000,log10(Cu[loc.in]),spar=0.8),col=1,lwd=1.3)
@
\begin{figure}
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<label=fig8,fig=TRUE,echo=TRUE,width=9,height=4.5>>=
<<fig8plot>>
@
\end{center}
\caption{The subarea includes Nikel/Zapoljanij and Monchegorsk. The refernce point (dot) is in the north-west part of the survey area (left). Right: The spatial distance plot and the concentrations in the subarea.}
\label{fig:eight}
\end{figure}

Another interessting plot is the so called ternary diagnram. In this diagram it is possible to compare three variables in just one graphic. It is important to mention that the relative proportion is plotted and not the absolut value. It is clear that the three variables should sum up to 100 percent to be plotted in the diagram and it is important that the three elements have the same order of magnitude. Otherwise the points will be concentrated in one corner. If the difference between the variables is to high, they should be transformed so that they are in the same data range. Figure~\ref{fig:nine} shows a ternary plot of Al-Fe-Mn of the Kola project moss data. We can see that most of the points tend to the Mn corner of the diagram.

<<label=fig9plot,include=FALSE, echo=FALSE>>=
par(mfrow=c(1,1))
x=moss[,c("Al","Fe","Mn")]
ternary(x,grid=TRUE,pch=3,cex=0.7,col=1)
text(0.21,0.26,"563")
text(0.1,0.05,"726")
text(0.21,0.06,"325")
@

\begin{figure}
\begin{center}
<<label=fig9,fig=TRUE,echo=TRUE,width=7,height=7>>=
<<fig9plot>>
@
\end{center}
\caption{Ternary plot for selected elements of the moss data set.}
\label{fig:nine}
\end{figure}

\chapter{Comparing Data in Tables and Graphics}
In this chapter we go back to our standard analysis methods. Sometimes it is not so important to examine a relation betweenn different elements but it can be important to analyse the change in the concentration of one element over the time. First of all the user has to ask if there are differences and what causes the differences. The next question is if the difference is significant in a statistical way. In the Kola project the change over the time is not so important but it is interessting to analyse the concentration in the different parts of the ecosystem. The data set of the project consists of four layers: moss, O-horizon, B-horizon and C-horizon. For detailed information about the different layers we recommend to read Chapter 1 of the book.

Now we are able to produce some interesting plots of the four layers of the ecosystem for one element (Al). Figure~\ref{fig:ten} displays the density traces of Al in all four sample materials. The data were log-transformed to make them more symmetric.

<<label=fig10plot,include=FALSE,echo=FALSE>>=
data(ohorizon)
data(bhorizon)

nam="Al"
M=density(log10(moss[,nam]))
O=density(log10(ohorizon[,nam]))
B=density(log10(bhorizon[,nam]))
C=density(log10(chorizon[,nam]))
plot(0,0,xlim=c(min(M$x,O$x,B$x,C$x),max(M$x,O$x,B$x,C$x)),
  ylim=c(min(M$y,O$y,B$y,C$y),max(M$y,O$y,B$y,C$y)),
  main="",xlab=paste(nam," [mg/kg]",sep=""),ylab="Density",
  xaxt="n",cex.lab=1.2,type="n")
axis(1,at=log10(a<-sort(c((10^(-50:50))%*%t(c(2,5,10))))),labels=a)
lines(M,lty=1)
lines(O,lty=2)
lines(B,lty=4)
lines(C,lty=5)
text(1.5,1.5,"Moss",pos=4)
text(2.5,1.45,"O-horizon",pos=4)
text(4.6,0.9,"B-horizon",pos=4,srt=90)
text(3.7,1.4,"C-horizon",pos=4,srt=90)
@

\begin{figure}
\begin{center}
<<label=fig10,fig=TRUE,echo=TRUE,width=7,height=7>>=
<<fig10plot>>
@
\end{center}
\caption{Density traces of Al in all four layers of the Kola data.}
\label{fig:ten}
\end{figure}

Another interesting graphic is a CP-plot (Cumulative Probability plot) of the data set. This can be made in the following way where every layer of the ecosystem is displayed with a different symbol. It can be a better solution to plot them with different colours.

<<label=fig11plot,include=FALSE,echo=FALSE>>=
nam="Al"
M=log10(moss[,nam])
O=log10(ohorizon[,nam])
B=log10(bhorizon[,nam])
C=log10(chorizon[,nam])
qpplot.das(M,qdist=qnorm,xlab=paste(nam," [mg/kg]",sep=""),
ylab="Probability [%]", pch=3,cex=0.7, logx=TRUE,
logfinetick=c(2,5,10),logfinelab=c(2,5,10),line=F,cex.lab=1.2,
xlim=c(min(M,O,B,C),max(M,O,B,C)))
points(sort(O),qnorm(ppoints(length(O))),pch=4,cex=0.7)
points(sort(B),qnorm(ppoints(length(B))),pch=1,cex=0.7)
points(sort(C),qnorm(ppoints(length(C))),pch=22,cex=0.7)
legend("topleft",legend=c("Moss","O-horizon","B-horizon","C-horizon"),
       pch=c(3,4,1,22),bg="white")
@

\begin{figure}
\begin{center}
<<label=fig11,fig=TRUE,echo=TRUE,width=7,height=7>>=
<<fig11plot>>
@
\end{center}
\caption{CP-plot of Al in the different sample layers.}
\label{fig:eleven}
\end{figure}

The last method to display a statistical information of the Al element in the four layers is the boxplot. This graphic has the advantage that it provides a statistical summary and that it makes only minor use of a model (just for calculating the ends of the whiskers).

<<label=fig12plot,include=FALSE,echo=FALSE>>=
nam="Al"
M=moss[,nam]
O=ohorizon[,nam]
B=bhorizon[,nam]
C=chorizon[,nam]
boxplotlog(C,B,O,M,horizontal=T,xlab=paste(nam," [mg/kg]",sep=""),cex=0.7,cex.axis=1.2,
   log="x",cex.lab=1.2,pch=3,names=c("C-hor","B-hor","O-hor","Moss"),xaxt="n")
axis(1,at=(a<-sort(c((10^(-50:50))%*%t(c(2,5,10))))),labels=a)
@

\begin{figure}
\begin{center}
<<label=fig12,fig=TRUE,echo=TRUE,width=7,height=7>>=
<<fig12plot>>
@
\end{center}
\caption{Log-boxplot (because of the problems mentioned in Chapter 2) of Al in the four different layers of the ecosystem.}
\label{fig:twelve}
\end{figure}

In Figure~\ref{fig:twelve} it is visible that there are many upper outliers in moss. The number of outliers is lower in the other sample layers.

It is also possible to make such a comparison of the spatial data structure. Another type of comparison is a graphic (does not matter if boxplot, density trace or something else) of the same element in the same layer but in different countries. It is also feasible to make a scatterplot of two elements for the different countries. Instead of a graphic the user can make a table for the comparison. The problem is that a table contains not as much information because very soon it becomes incomprehensible. All plots presented in the previous chapters can be made to compare the different parts of the system or different countries. In this chapter only the three basic methods were presented. 

\chapter{Correlation}
Before we can talk about correlation we have to introduce the covariance and the difference between these two measures. The covariance can take any number and therefore only the sign is informative but the strength of the relation can not be interpreted. The reason is because the covariance depends on the variability of the considered variables. The correlation eliminates this dependency on the variability which means that with the correlation coefficient it is possible to say something about the strength of the relation between the variables.

The correlation results in a number between -1 and +1. $\pm1$ shows a perfect relation and 0 means no systematic relation. There are different methods for computing the correlation. Well known are the Pearson, Spearman and Kendall methods. In this chapter we will only talk about the Pearson correlation because this is ideal when the data follow a bivariate normal distribution. The problem is that for geochemical data this is not very often reality. Therefore the data have to be log-transformed (as mentioned in Chapter 2) before calculating the Pearson correlation coefficients. The methods of Spearman and Kendall have the advantage that they do not depend on a distribution. Another advantage is that they do not look for linear relation, but if the relation is monotonically increasing or decreasing. The difference between those two methods is in the way they compute the correlation.

A more robust estimation of the correlation is the Minimum Covariance Determinant (MCD) estimator which searches for the most compact ellipsoid containing e.g. 50 percent of the data points. The user can choose any percentage between 50 and 100 percent. If the user goes near 100 percent, the robustness of the estimator is lost. Figure~\ref{fig:thirteen} demonstrates that the Pearson correlation and the MCD estimation deliver different results.

<<label=fig13plot,include=FALSE, echo=FALSE>>=
x=chorizon[,c("Ca","Cu","Mg","Na","P","Sr","Zn")]
par(mfrow=c(1,1),mar=c(4,4,2,0))
R=covMcd(log10(x),cor=T)$cor
P=cor(log10(x))
CorCompare(R,P,labels1=dimnames(x)[[2]],labels2=dimnames(x)[[2]],
method1="Robust",method2="Pearson",ndigits=2, cex.label=1.2)
@

\begin{figure}
\begin{center}
<<label=fig13,fig=TRUE,echo=TRUE,width=7,height=7>>=
<<fig13plot>>
@
\end{center}
\caption{Correlation coefficients and correlation ellipses. MCD-based (upper number) and Pearson (lower number) correlation.}
\label{fig:thirteen}
\end{figure}

\chapter{Multivariate Graphics}
Most of the graphics presented in the last chapters displays up to two variables of the data set. In a map it is only possible to visualise one variable at the time. But sometimes it is necessary to study several variables at the same time. This is the reason why multivariate graphics have been developed. In this chapter a small selection of procedures is illustrated and at the end one graphic is displayed. One possibility to represent multivariate data is in a table where each observation is identified by its sample number. The problem of a multivariate graphic is that it needs more space than a single symbol and this reduces the number of observations that can be shown in a map. It is important that the data is transformed to reduce the influence of outliers and to get a more symmetrical distribution. Then it has to be range transformed to [0,1] so that each variable has equal influence.

After all these transformations the plot can be made. There are different methods to plot multivariate graphics such as:
\begin{itemize}
\item Profiles: The selected variable is shown on the $x$-axis and the observation (log- and range-transformed) is plotted on the $y$-axis. A line connects the values.
\item Stars: Instead of plotting the variable along the $x$-axis it is possible to plot it radially with equal angle. The length of the line is proportional to the value. The graphic can be plotted in three different styles: as radii, as polygons drawn around the ends of the radii and as polygon alone.
\item Segments: This method is closley related to stars. Each variable is represented by a segment and the area of each segment represents the value of the sample.
\item Boxes: First the variables undergo a cluster analysis. Each cluster corresponds to one side of the box. This method is only applicable when the variables can be grouped in three clusters. The dimension of each axis is proportional to the sum of the values.
\item Trees: The computation is similar to boxes. The advantage is that the variables are not forced to be grouped into three clusters. The result is the same as for a dendogram from hierarchical cluster analysis. The height of the battlements or length of the branches represent the values of the variables.
\end{itemize}

<<label=fig14plot,include=FALSE, echo=FALSE>>=
X=ohorizon[,"XCOO"]
Y=ohorizon[,"YCOO"]
el=log10(ohorizon[,c("Co","Cu","Ni","Rb","Bi","Na","Sr")])
data(kola.background)
sel <- c(3,8,22, 29, 32, 35, 43, 69, 73 ,93,109,129,130,134,168,181,183,205,211,
      218,237,242,276,292,297,298,345,346,352,372,373,386,408,419,427,441,446,490,
      516,535,551,556,558,564,577,584,601,612,617)
x=el[sel,]
dimnames(x)[[1]]=ohorizon[sel,1]
xwid=diff(range(X))/12e4
ywid=diff(range(Y))/12e4
par(mfrow=c(1,2),mar=c(1.5,1.5,1.5,1.5))
plot(X,Y,frame.plot=FALSE,xaxt="n",yaxt="n",xlab="",ylab="",type="n",
   xlim=c(360000,max(X)))
plotbg(map.col=c("gray","gray","gray","gray"),add.plot=T)
tree(x,locations=cbind(X[sel],Y[sel]),len=700,key.loc=c(793000,7760000),leglen=1500,
     cex=0.75, add=T, leglh=6,lh=30,wmax=120,wmin=30,labels=NULL)
scalebar(761309,7373050,861309,7363050,shifttext=-0.5,shiftkm=37e3,sizetext=0.8)
Northarrow(362602,7818750,362602,7878750,362602,7838750,Alength=0.15,Aangle=15,Alwd=1.3,Tcex=1.6)
par(mar=c(.1,.1,.1,.5))
tree(x,key.loc=c(15,0),len=0.022, lh=30,leglh=4,
    wmax=120,wmin=30, leglen=0.05, ncol=8, cex=0.75)
@

\begin{figure}
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<label=fig14,fig=TRUE,echo=TRUE,width=9,height=4.5>>=
<<fig14plot>>
@
\end{center}
\caption{Trees constructed for 49 selected samples from Kola O-horizon soil data set.}
\label{fig:fourteen}
\end{figure}

\chapter{Multivariate Outlier Detection}
The detection of data outliers and unusual data structures is very important in the statistical analysis of data. The basis for an outlier is the location and spread of the data. Outliers typically have a large distance to the center of the sample and therefore it is important to find a threshold, dividing the observations into background and outlier. This problem has received much attention in the literature, especialy in environmental sciences.

When we compare univariate and multivariate outlier detection we can see that there is an essential difference. In the univariate case a box (for example all observations except the lower and upper 2 percent) is defined. All values outside the box are characterized as outliers. The problem is that in this case the multivariate, often elliptical, structure of the data set is ignored. Therefore a better solution is to estimate ellipses (ellipsoids) to defined quantiles. All values outside the ellipse are outliers. Now there appears a new problem which is, how to estimate the central location and covariance. We have talked about the difference between robust and non-robust estimation in Chapter 3. Different methods result in different ellipses and therefore different characterizations.

Now it is important to find the multivariate outlier boundary. To demonstrate the problem we take seven elements of the Kola moss data and search for outliers. This results in Figure~\ref{fig:fifteen} (left) where all outliers are displayed. As one can see there are outliers near the Russian nickel industry and in northwest Norway and Finland which is one of the most pristine parts of the survey area. Therefore it is necessary to distinguish between those areas and this is the reason why it is important to take a closer look at the distribution of the robust Mahalanobis distances which are based on a robust estimation of the central value and covariance. We additionally provide a coding (grey scale) for the magnitude of the average element concentration. Now we get different types of outliers. The outliers near the Russian nickel industry are identified as ``high'' and the outliers in Norway and Finland are marked as low average element concentrations.

<<label=fig15plot,include=FALSE,echo=FALSE>>=
X=moss[,"XCOO"]
Y=moss[,"YCOO"]
xwid=diff(range(X))/12e4
ywid=diff(range(Y))/12e4
par(mfrow=c(1,2),mar=c(1.5,1.5,1.5,1.5))
el=c("Ag","As","Bi","Cd","Co","Cu","Ni")
dat=log10(moss[,el])
res <- plotmvoutlier(cbind(X,Y),dat,symb=F,map.col=c("grey","grey","grey","grey"),
       map.lwd=c(1,1,1,1),
       xlab="",ylab="",frame.plot=FALSE,xaxt="n",yaxt="n")    
legend("topright",pch=c(3,21),pt.cex=c(0.7,0.2), legend=c("outliers",
"non-outliers"), cex=0.8)
scalebar(761309,7373050,861309,7363050,shifttext=-0.5,shiftkm=37e3,sizetext=0.8)
Northarrow(362602,7818750,362602,7878750,362602,7838750,Alength=0.15,Aangle=15,Alwd=1.3,Tcex=1.6)
res <- plotmvoutlier(cbind(X,Y),dat,symb=T,bw=T,map.col=c("grey","grey","grey","grey"),
       map.lwd=c(1,1,1,1), lcex.fac=0.6,
       xlab="",ylab="",frame.plot=FALSE,xaxt="n",yaxt="n")    
perc.ypos=seq(from=77.4e5,to=78.7e5,length=6)
sym.ypos=perc.ypos[-6]+diff(perc.ypos)/2
points(rep(8.2e5,5),sym.ypos,pch=c(1, 1, 16, 3, 3),cex=c(1.5, 1, 0.5, 1, 1.5)*0.6)
text(rep(8.0e5,6),perc.ypos[-5],round(100*c(0,0.25,0.5,0.75,1),2),pos=2,cex=0.7)
text(8.0e5,perc.ypos[5],"break",cex=0.72,pos=2)
text(7.5e5,79e5,"Percentile",cex=0.8)
text(8.4e5,79e5,"Symbol",cex=0.8)
scalebar(761309,7373050,861309,7363050,shifttext=-0.5,shiftkm=37e3,sizetext=0.8)
Northarrow(362602,7818750,362602,7878750,362602,7838750,Alength=0.15,Aangle=15,Alwd=1.3,Tcex=1.6)
@

\begin{figure}
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<label=fig15,fig=TRUE,echo=TRUE,width=9,height=4.5>>=
<<fig15plot>>
@
\end{center}
\caption{Identifying multivariate outliers in the Kola moss data. The left map shows the location of the outliers and the right one includes information on the relative distance to the multivariate data center.}
\label{fig:fifteen}
\end{figure}
 

\chapter{Principal Component Analysis and Factor Analysis}
Here we will try to point out the differences between Principal Component Analysis (PCA) and Factor Analysis (FA). The main difference is that PCA focuses on maximum variance of the resulting components and FA accounts for maximum intercorrelation. The FA-model allows the common factor not to explain the total variability of the data and the existence of a unique factor which behaves completely different than the majority of the other factors. PCA always tries to accomodate the total structure of the data. Although FA is better suited to detect common factors, PCA is used more frequently in environmental sciences. Another difference is that in the case of FA the number of factors must be defined in advance. The aim of PCA and FA is dimension reduction. In applied geochemistry it is important to find e.g. two variables which describe much of the variability of the complete data set. These are the first two principal components. The problem is that the interpretation of the two components is very difficult because they are determined by a maximum variance criterion. To expand interpretability it can be useful to rotate the principal components. For more details we refer to Chapter 14 of the book.

The basis for PCA is the covariance matrix of the data set and now we have the same problem as many times before. How to estimate the covariance? If we take the classical approach the estimation can be influenced by outliers. Therefore we make a robust estimation of the covariance using the MCD estimator. Figure~\ref{fig:sixteen} displays the first two principal components based on a robust estimation of the covariance. In the graphic the influence of the outliers is reduced. When a plot of PCA with a non-robust estimation will be made it is visible that the components were influenced by the outlier.

Note that geochemical data are compositional data. The real information is in the ratios, and not in the absolute values. Especially for a multivariate analysis it is thus advisable to use an appropriate transformation. We used the isometric logratio transformation which accounts for the compositional nature of the data.

<<label=fig16plot,include=FALSE,echo=TRUE>>=
par(mfrow=c(1,1))
X=moss[,"XCOO"]
Y=moss[,"YCOO"]
var=c("Ag","Al","As","B","Ba","Bi","Ca","Cd","Co","Cr","Cu","Fe","Hg","K","Mg","Mn","Mo",
      "Na","Ni","P","Pb","Rb","S","Sb","Si","Sr","Th","Tl","U","V","Zn")
x=moss[,var]
ilr <- function(x){
x.ilr=matrix(NA,nrow=nrow(x),ncol=ncol(x)-1)
  for (i in 1:ncol(x.ilr)){
    x.ilr[,i]=sqrt((i)/(i+1))*log(((apply(as.matrix(x[,1:i]), 1, prod))^(1/i))/(x[,i+1]))
  }
return(x.ilr)
}
invilr <- function(x.ilr,x.ori){
y=matrix(0,nrow=nrow(x.ilr),ncol=ncol(x.ilr)+1)
for (i in 1:(ncol(y)-1)){
    for (j in i:ncol(x.ilr)){
      y[,i]=y[,i]+x.ilr[,j]/sqrt(j*(j+1))
    }
}
for (i in 2:ncol(y)){
   y[,i]=y[,i]-sqrt((i-1)/i)*x.ilr[,i-1]
}
yexp=exp(y)
x.back=yexp/apply(yexp,1,sum)
return(x.back)
}
x2=ilr(x)
res2=princomp(x2,cor=TRUE)
x2.mcd=covMcd(x2,cor=T)
res2rob=princomp(x2,covmat=x2.mcd,cor=T)
V=matrix(0,nrow=31,ncol=30)
for (i in 1:ncol(V)){
  V[1:i,i] <- 1/i
  V[i+1,i] <- (-1)
  V[,i] <- V[,i]*sqrt(i/(i+1))
}
xgeom=10^apply(log10(x),1,mean)
xlc1=x/xgeom
xlc=log10(xlc1)
reslc=princomp(xlc,cor=TRUE)
res2robback=res2rob
res2robback$loadings <- V%*%res2rob$loadings
res2robback$scores <- res2rob$scores%*%t(V)
dimnames(res2robback$loadings)[[1]] <- names(x)
res2robback$sdev <- apply(res2robback$scores,2,mad)
biplot(res2robback,xlab="PC1 (robust)",ylab="PC2 (robust)",col=c(gray(0.6),1),
       xlabs=rep("+",nrow(x)),cex=0.8)
@

\begin{figure}
\begin{center}
<<label=fig16,fig=TRUE,echo=FALSE,width=7,height=7>>=
<<fig16plot>>
@
\end{center}
\caption{Biplot of robust PCA.}
\label{fig:sixteen}
\end{figure}

\chapter{Cluster Analysis}
The aim of cluster analysis is to divide the data set into groups. Within one cluster the samples should be very similar and observations in different clusters should be very dissimilar. As one can imagine some problems appear. One question is the number of groups the data should be split and another one is how to define similarity. It is also a problem that different methods give different results and if one variable is added or deleted to/from the data set the result can change drastically. Many techniques are based on a distance measure and this has the great advantage that there are a priori no statistical assumptions. Modern software implementation of clustering can calculate with different distance measures but the ``favourites'' are the Eulidean and the Manhattan distances.

It is important to mention that clustering is no proof of a relationship between the variables or samples. In theory it can be useful to start with cluster anlaysis and then to make further statistical anaylses with the homogeneous data sets. 

Important clustering techniques are:
\begin{itemize}
\item hierarchical methods
\item partitioning methods
\item model based methods
\item fuzzy methods
\end{itemize}
The hierarchical method starts with a distance matrix and defines every sample as one cluster. Then the clusters will be combined stepwise. Another possibility is to go the other way round which means to start with one cluster (includes all observations) and then split this cluster stepwise. We will concentrate on the method which starts with many clusters (every sample is one cluster). In this version the number of clusters is reduced one by one because in every step two clusters are combined. At the end one single cluster is left which includes all samples. The best known methods to link two clusters are average linkage (average of all pairs of distances between the samples in the group), complete linkage (looks for the maximum distance between two clusters) and single linkage (the minimum distance between two clusters). When one of these linkage methods is computed for all cluster combinations the combination with the minimum distance is chosen. The visualization of this technique is called dendrogram.\\
\\

For hierarchical methods it is not necessary to define the number of clusters in advance. When using partitioning methods the number of clusters must be pre-determind. To find out in how many groups the data set should be splitted it can be useful to start with a hierarchical cluster analysis. A very popular algorithm for partitioning is the k-means algorithm. k-means starts with $k$ given initial cluster centroids and then the observations are assigned to the nearest centroid. Now the centroids are recomputed and again the observations are assigned to the nearest centroid. This algorithm is iterated until convergence.\\
\\

The former two methods are based on the distance measure in contrast to model based methods. The difference is that model based methods describe the shape of the possible clusters. The algorithm for this technique is Mclust which selects the cluster models (e.g. spherical or elliptical). Then the algorithm determines the cluster membership for all samples over a range of different numbers of clusters. Finally the combination of model and number of groups with the highest BIC (Bayesian Information Criterion) can be chosen.\\
\\

The last techniques are the fuzzy methods. In this case the observations are not allocated to one group. Instead, every observation has a probability to be assigned to each cluster and this probability can be represented in different grey-scales. A popular fuzzy clustering algorithm is the fuzzy c-means (FCM) algorithm. For this algorithm the number of clusters has to be determined by the user. Then the cluster centroids and the membership coefficients are calculated. Figure~\ref{fig:seventeen} shows the results of fuzzy clustering for Cu, Mg, Ni, and Sr of the moss data in 4 different cluster.

<<label=fig17plot,include=FALSE,echo=TRUE>>=
el=c("Cu","Ni","Mg","Sr")
x=scale(log10(moss[,el]))
set.seed(100)
res=cmeans(x,4)
X=moss[,"XCOO"]
Y=moss[,"YCOO"]
xwid=diff(range(X))/12e4
ywid=diff(range(Y))/12e4
par(mfrow=c(2,2),mar=c(1.5,1.5,1.5,1.5))
plot(X,Y,frame.plot=FALSE,xaxt="n",yaxt="n",xlab="",ylab="",type="n")
plotbg(map.col=c("gray","gray","gray","gray"),add.plot=T)
points(moss[,"XCOO"],moss[,"YCOO"],col=gray(1-res$mem[,1]),pch=15,cex=1)
text(752000,7880000,"Cluster 1",cex=1.1)
scalebar(761309,7373050,861309,7363050,shifttext=-0.5,shiftkm=37e3,sizetext=0.8)
Northarrow(362602,7818750,362602,7878750,362602,7838750,Alength=0.15,Aangle=15,Alwd=1.3,Tcex=1.6)
plot(X,Y,frame.plot=FALSE,xaxt="n",yaxt="n",xlab="",ylab="",type="n")
plotbg(map.col=c("gray","gray","gray","gray"),add.plot=T)
points(moss[,"XCOO"],moss[,"YCOO"],col=gray(1-res$mem[,2]),pch=15,cex=1)
text(752000,7880000,"Cluster 2",cex=1.1)
scalebar(761309,7373050,861309,7363050,shifttext=-0.5,shiftkm=37e3,sizetext=0.8)
Northarrow(362602,7818750,362602,7878750,362602,7838750,Alength=0.15,Aangle=15,Alwd=1.3,Tcex=1.6)
plot(X,Y,frame.plot=FALSE,xaxt="n",yaxt="n",xlab="",ylab="",type="n")
plotbg(map.col=c("gray","gray","gray","gray"),add.plot=T)
points(moss[,"XCOO"],moss[,"YCOO"],col=gray(1-res$mem[,3]),pch=15,cex=1)
text(752000,7880000,"Cluster 3",cex=1.1)
scalebar(761309,7373050,861309,7363050,shifttext=-0.5,shiftkm=37e3,sizetext=0.8)
Northarrow(362602,7818750,362602,7878750,362602,7838750,Alength=0.15,Aangle=15,Alwd=1.3,Tcex=1.6)
plot(X,Y,frame.plot=FALSE,xaxt="n",yaxt="n",xlab="",ylab="",type="n")
plotbg(map.col=c("gray","gray","gray","gray"),add.plot=T)
points(moss[,"XCOO"],moss[,"YCOO"],col=gray(1-res$mem[,4]),pch=15,cex=1)
text(752000,7880000,"Cluster 4",cex=1.1)
scalebar(761309,7373050,861309,7363050,shifttext=-0.5,shiftkm=37e3,sizetext=0.8)
Northarrow(362602,7818750,362602,7878750,362602,7838750,Alength=0.15,Aangle=15,Alwd=1.3,Tcex=1.6)
@

\begin{figure}
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<label=fig17,fig=TRUE,echo=FALSE>>=
<<fig17plot>>
@
\end{center}
\caption{Result for FCM into four cluster for Cu, Mg, Ni and Sr in the moss data (log-transformed and standardised).}
\label{fig:seventeen}
\end{figure}

\chapter{Regression Analysis}
For regression analysis dependent and independent variables are used. The dependent variable $y$ should be predicted using the information of the independent variables. Regression analysis is very important when it is expensive to analyse one element, or if the quality of the element is very poor. Now it could be useful to predict this element from all the other variables. In practise it is necessary to know some values of the $x$ and $y$ variable to fit any model. Often only a linear model is fitted and the chosen criterion is the least squares (LS) criterion. Least squares means that the sum of the squared errors (=residuals) should be minimised. The resulting parameters define the orientation of the line or curve (when a quadratic model is computed). A robust counterpart to the LS criterion is least trimmed sum of squares (LTS) which downweights the largest residuals. Regression analysis has some requirements to the data input.
\begin{itemize}
\item Homogeneity: The variance of the dependent variables across the range of the data should be similar.
\item Normality: The residuals should be normally distributed.
\item Data outliers, extreme values: The presence of data outliers can influence the result because they can ``lever'' the regression line.
\item Correlation: The base for regression analysis is that the dependent and independent variables are correlated. For environmental studies it can happen that when the data is spatially correlated a relationship is over-represented.
\item Dimensionality: It is important that the number of observations is larger than the number of independent variables. If this is not the case, other methods have to be employed.
\end{itemize}
We mentioned the problem of outliers in a regression model because they can influence the regression line or hyperplane. Therefore it is interesting to find a robust method for estimating the regression coefficients. As mentioned above, a possibility is to use least trimmed sum of squares (LTS) which downweights the larger residuals. After fitting an LTS model we can make some checks if the assumptions are fulfilled. With a plot, where the $x$-axis represents the robust Mahalanobis distances and the $y$-axis the values of the residuals, it is possible to identify outliers. 

<<label=fig18plot,include=FALSE,echo=TRUE,results=hide>>=
attach(chorizon)
X=chorizon[,"XCOO"]
Y=chorizon[,"YCOO"]
xdat=log10(chorizon[,101:109]/chorizon[,110])
set.seed(101)
xdat.mcd=covMcd(xdat,cor=T)
md=sqrt(xdat.mcd$mah)
crit=sqrt(qchisq(0.975,ncol(xdat)))
set.seed(104)
res=ltsReg(log10(Be) ~ Al_XRF+Ca_XRF+Fe_XRF+K_XRF+Mg_XRF+Mn_XRF+Na_XRF+P_XRF+Si_XRF,
   data=xdat)
resid=res$residuals/res$scale
xwid=diff(range(X))/12e4
ywid=diff(range(Y))/12e4
par(mfrow=c(2,2),mar=c(4,4,2,2))
psymb=res$lts.wt
psymb[res$lts.wt==1] <- 1
psymb[res$lts.wt==0] <- 3
pcex=res$lts.wt
pcex[res$lts.wt==1] <- 1.3
pcex[res$lts.wt==0] <- 0.8
qqnorm(resid,xlab="Quantiles of standard normal distribution",ylab="Standardised LTS residuals",
   pch=psymb,cex=pcex,main="",cex.lab=1.2)
qqline(resid)
plot(res$fitted,resid,cex.lab=1.2,xlab="Fitted values",ylab="Standardised LTS residuals",type="n")
points(res$fitted[res$lts.wt==0],resid[res$lts.wt==0],cex=0.8,pch=3)
points(res$fitted[res$lts.wt==1],resid[res$lts.wt==1],cex=0.8,pch=1)
abline(h=0,col="grey",lty=2)
abline(h=c(-2.5,2.5),lty=3,cex=1.1)
symb.nor=16 #1
symb.resl=1
symb.resh=22
symb.goodl=3 #16
symb.badll=1 #16
symb.badlh=15
plot(md,resid,cex=0.5,pch=3,type="n",xlab="Robust Mahalanobis distances",
        ylab="Standardised LTS residuals", cex.lab=1.2)
abline(h=c(2.5,-2.5))
abline(v=crit)
md.resid=as.data.frame(cbind(md,resid))
points(md.resid[md<crit & abs(resid)<2.5,], cex=0.3,pch=symb.nor)
points(md.resid[md<crit & resid>=2.5,], cex=0.9,pch=symb.resh)
points(md.resid[md<crit & resid<=(-2.5),], cex=0.9,pch=symb.resl)
points(md.resid[md>=crit & abs(resid)<2.5,], cex=0.6,pch=symb.goodl)
points(md.resid[md>=crit & resid>=2.5,], cex=0.9,pch=symb.badlh)
points(md.resid[md>=crit & resid<=(-2.5),], cex=1.1,pch=symb.badll)
par(mar=c(1.5,1.5,1.5,1.5))
XY=cbind(X,Y)
plot(X,Y,frame.plot=FALSE,xaxt="n",yaxt="n",xlab="",ylab="",type="n")
plotbg(map.col=c("gray","gray","gray","gray"),add.plot=T)
points(XY[md<crit & abs(resid)<2.5,], cex=0.3,pch=symb.nor)
points(XY[md<crit & resid>=2.5,], cex=0.9,pch=symb.resh)
points(XY[md<crit & resid<=(-2.5),], cex=0.9,pch=symb.resl)
points(XY[md>=crit & abs(resid)<2.5,], cex=0.6,pch=symb.goodl)
points(XY[md>=crit & resid>=2.5,], cex=0.9,pch=symb.badlh)
points(XY[md>=crit & resid<=(-2.5),], cex=1.1,pch=symb.badll)
legend("topright",pch=c(symb.nor,symb.resh,symb.resl,symb.goodl,symb.badlh,symb.badll),
  pt.cex=c(0.3,0.9,0.9,0.6,0.9,1.1), 
legend=c("Normal observations","High vertical outliers","Low vertical outliers",
"Good leverage points","High bad leverage points","Low bad leverage points"), cex=0.7)
scalebar(761309,7373050,861309,7363050,shifttext=-0.5,shiftkm=37e3,sizetext=0.8)
Northarrow(362602,7818750,362602,7878750,362602,7838750,Alength=0.15,Aangle=15,Alwd=1.3,Tcex=1.6)
@

\begin{figure}
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<label=fig18,fig=TRUE,echo=FALSE,width=9,height=9,results=hide>>=
<<fig18plot>>
@
\end{center}
\caption{Robust regression diagnostics for the Be variable. QQ-plot (upper left), LTS residuals versus fitted (upper right), standardised LTS residuals versus Mahalanobis distances (lower left), map indicating the location and characterization of the outliers (lower right)}
\label{fig:eighteen}
\end{figure}

In Figure~\ref{fig:eighteen} (lower left) we can see a plot of standardised LTS residuals versus Mahalanobis distances. The samples can be divided into six groups were all observation with low absolute residuals and Mahalanobis distances have low leverage. Only a little leverage on a classical regression model comes from the samples with high absolut residuals and low Mahalanobis distances. Interessting are those variables which have a high Mahalanobis distance because they exert leverage on the model. Those with low absolut residual are good leverage points because they stabilize the regression line or hyperplane.


\end{document}
